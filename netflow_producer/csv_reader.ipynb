{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc5872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "# import json\n",
    "from kafka import KafkaProducer\n",
    "from pathlib import Path\n",
    "# import logging\n",
    "from loguru import logger\n",
    "from typing import Any, Dict\n",
    "import pytz\n",
    "\n",
    "current_timezone = pytz.timezone('Europe/Moscow')\n",
    "\n",
    "# Конфигурация\n",
    "# KAFKA_BROKER = 'localhost:9092'\n",
    "# KAFKA_TOPIC = 'csv_data_topic'\n",
    "# CSV_DIRECTORY = '../data/TelecomX/telecom100k/'\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logger.add(sys.stdout, colorize=True, format=\"<green>{time}</green> <level>{message}</level>\")\n",
    "# logger.add(\"netflow_producer.log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3b333f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    model_config = SettingsConfigDict(env_ignore_empty=True, env_file='.env', env_file_encoding='utf-8')\n",
    "\n",
    "    current_time: str = 'Europe/Moscow'\n",
    "    kafka_broker: str = 'localhost:9092'\n",
    "    kafka_topic: str = 'csv_data_topic'\n",
    "    csv_directory: str = '../data/TelecomX/telecom100k/'\n",
    "    log_file: str = \"logs/netflow_producer.log\"\n",
    "    time_pointer_file: str = 'logs/time_pointer.txt'\n",
    "\n",
    "\n",
    "settings = Settings()\n",
    "KAFKA_BROKER = settings.kafka_broker\n",
    "KAFKA_TOPIC = settings.kafka_topic\n",
    "CSV_DIRECTORY = settings.csv_directory\n",
    "\n",
    "# LOG_FILE = settings['log_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df0625a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.add(settings.log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3730fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kafka_producer() -> KafkaProducer:\n",
    "    \"\"\"\n",
    "    Инициализируем KafkaProducer с базовыми настройками.\n",
    "    В продакшене нужно добавить обработку ошибок подключения, настройки безопасности и т.д.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        producer = KafkaProducer(\n",
    "            bootstrap_servers=KAFKA_BROKER,\n",
    "            # value_serializer=lambda v: json.dumps(v, allow_nan=False).encode('utf-8'),\n",
    "            value_serializer=lambda v: str(v).encode('utf-8'),\n",
    "            retries=5,               # Повторная отправка при сбоях\n",
    "            linger_ms=10,            # Небольшая задержка перед отправкой\n",
    "            max_request_size=1048576 # Ограничение размера запроса (1MB)\n",
    "        )\n",
    "        logger.info(\"KafkaProducer успешно инициализирован!\")\n",
    "        return producer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка инициализации KafkaProducer: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4945ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r0_logs = sorted(Path(CSV_DIRECTORY).glob('psx_66.1_*.txt'))\n",
    "r1_logs = sorted(Path(CSV_DIRECTORY).glob('psx_66.2_*.txt'))\n",
    "r2_logs = sorted(Path(CSV_DIRECTORY).glob('psx_66.3_*.txt'))\n",
    "r3_logs = sorted(Path(CSV_DIRECTORY).glob('psx_62.0_*.csv'))\n",
    "r4_logs = sorted(Path(CSV_DIRECTORY).glob('psx_69.0_*.csv'))\n",
    "r5_logs = sorted(Path(CSV_DIRECTORY).glob('psx_65.0_*.csv'))\n",
    "\n",
    "# r3_logs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "383fc7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1564 entries, 0 to 1563\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype                        \n",
      "---  ------        --------------  -----                        \n",
      " 0   IdSession     1564 non-null   int64                        \n",
      " 1   IdPSX         1564 non-null   int64                        \n",
      " 2   IdSubscriber  1564 non-null   int64                        \n",
      " 3   StartSession  1564 non-null   datetime64[ns, Europe/Moscow]\n",
      " 4   EndSession    20 non-null     datetime64[ns, Europe/Moscow]\n",
      " 5   Duration      1564 non-null   int64                        \n",
      " 6   UpTx          1564 non-null   int64                        \n",
      " 7   DownTx        1564 non-null   int64                        \n",
      "dtypes: datetime64[ns, Europe/Moscow](2), int64(6)\n",
      "memory usage: 97.9 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37142/553117834.py:3: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df0 = pd.read_csv(csv_file0, parse_dates=['StartSession','EndSession'],sep='|').rename(columns={\"Duartion\": \"Duration\"})\n"
     ]
    }
   ],
   "source": [
    "# csv_file1 = Path(CSV_DIRECTORY+'psx_65.0_2024-01-04 08:20:00.csv');\n",
    "csv_file0 = r0_logs[0]\n",
    "df0 = pd.read_csv(csv_file0, parse_dates=['StartSession','EndSession'],sep='|').rename(columns={\"Duartion\": \"Duration\"})\n",
    "df0['StartSession'] = df0['StartSession'].dt.tz_localize('Etc/GMT-5').dt.tz_convert('Europe/Moscow')\n",
    "df0['EndSession'] = df0['EndSession'].dt.tz_localize('Etc/GMT-5').dt.tz_convert('Europe/Moscow')\n",
    "df0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f74fddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_time=Timestamp('2024-01-01 00:10:00+0300', tz='Europe/Moscow')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      2025-04-27 07:50:04.859693+03:00\n",
       "1      2025-04-27 08:08:47.859693+03:00\n",
       "2      2025-04-27 17:21:07.859693+03:00\n",
       "3      2025-04-27 18:47:28.859693+03:00\n",
       "4      2025-04-27 10:08:57.859693+03:00\n",
       "                     ...               \n",
       "1559   2025-04-27 18:41:39.859693+03:00\n",
       "1560   2025-04-27 17:00:57.859693+03:00\n",
       "1561   2025-04-27 16:03:10.859693+03:00\n",
       "1562   2025-04-27 12:06:53.859693+03:00\n",
       "1563   2025-04-27 18:06:17.859693+03:00\n",
       "Name: StartSession, Length: 1564, dtype: datetime64[ns, Europe/Moscow]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_time = pd.Timestamp(csv_file0.stem[9:], tz=current_timezone)\n",
    "print(f'{df_time=}')\n",
    "df_delta = datetime.now(tz=current_timezone) - df_time\n",
    "df0['StartSession'].add(df_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "abfb0555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Router(BaseModel):\n",
    "    IdSession: int\n",
    "    IdPSX: int\n",
    "    IdSubscriber: int\n",
    "    StartSession: datetime\n",
    "    EndSesstion: datetime | None = None\n",
    "    Duration: int\n",
    "    UpTx: int\n",
    "    DownTx: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7f84c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0.to_dict(orient='records')[0]\n",
    "# df0['StartSession']\n",
    "# Router(**df0.to_dict(orient='records')[0]).model_dump_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "185998d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-27 21:52:28.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_kafka_producer\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mKafkaProducer успешно инициализирован!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "producer = get_kafka_producer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1e4ebd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rec = Router(**df0.to_dict(orient='records')[0])\n",
    "# send_rec(rec, producer=producer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9feef5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dict_rec in df0.to_dict(orient='records'):\n",
    "#     rec = Router(**dict_rec)\n",
    "#     producer.send(KAFKA_TOPIC, value=rec.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2072d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def wait_until(target_time: datetime):\n",
    "    \"\"\"Ждёт до указанного времени (формат 'HH:MM:SS').\"\"\"\n",
    "    while True:\n",
    "        now = datetime.now()\n",
    "        if now >= target_time:\n",
    "            break  # Если время уже наступило, выходим\n",
    "        time_left = (target_time - now).total_seconds()\n",
    "        time.sleep(min(time_left, 1))  # Ждём не более 1 секунды за раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e74d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_dataframe(dataframe: pd.DataFrame, model: BaseModel, producer: KafkaProducer, headers: list=None):\n",
    "    for rec in dataframe.to_dict(orient='records'):\n",
    "        producer.send(KAFKA_TOPIC, value=model(**rec).model_dump_json(), headers=headers)\n",
    "    # producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7fa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-27 22:32:50.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mИтерация: 0, время: 2025-04-27 22:32:50.932841\u001b[0m\n",
      "\u001b[32m2025-04-27 22:32:50.938\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[34m\u001b[1mПропускаю время: 2024-01-01 00:10:00\u001b[0m\n",
      "\u001b[32m2025-04-27 22:32:50.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mИтерация: 1, время: 2025-04-27 22:32:50.932841\u001b[0m\n",
      "\u001b[32m2025-04-27 22:32:50.942\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[34m\u001b[1mПропускаю время: 2024-01-01 00:20:00\u001b[0m\n",
      "\u001b[32m2025-04-27 22:32:50.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mИтерация: 2, время: 2025-04-27 22:32:50.932841\u001b[0m\n",
      "\u001b[32m2025-04-27 22:32:50.945\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[34m\u001b[1mПропускаю время: 2024-01-01 00:30:00\u001b[0m\n",
      "\u001b[32m2025-04-27 22:32:50.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mИтерация: 3, время: 2025-04-27 22:32:50.932841\u001b[0m\n",
      "\u001b[32m2025-04-27 22:32:50.947\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[34m\u001b[1mПропускаю время: 2024-01-01 00:40:00\u001b[0m\n",
      "\u001b[32m2025-04-27 22:32:50.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mИтерация: 4, время: 2025-04-27 22:32:50.932841\u001b[0m\n",
      "\u001b[32m2025-04-27 22:32:50.950\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[34m\u001b[1mdf_file=PosixPath('../data/TelecomX/telecom100k/psx_66.1_2024-01-01 00:50:00.txt')\u001b[0m\n",
      "\u001b[32m2025-04-27 22:32:51.200\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[34m\u001b[1mdf_file=PosixPath('../data/TelecomX/telecom100k/psx_66.2_2024-01-01 00:50:00.txt')\u001b[0m\n",
      "\u001b[32m2025-04-27 22:32:51.486\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[34m\u001b[1mdf_file=PosixPath('../data/TelecomX/telecom100k/psx_66.3_2024-01-01 00:50:00.txt')\u001b[0m\n",
      "\u001b[32m2025-04-27 22:32:51.727\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[34m\u001b[1mdf_file=PosixPath('../data/TelecomX/telecom100k/psx_62.0_2024-01-01 00:50:00.csv')\u001b[0m\n",
      "\u001b[32m2025-04-27 22:32:51.939\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[34m\u001b[1mdf_file=PosixPath('../data/TelecomX/telecom100k/psx_69.0_2024-01-01 00:50:00.csv')\u001b[0m\n",
      "\u001b[32m2025-04-27 22:32:52.219\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[34m\u001b[1mdf_file=PosixPath('../data/TelecomX/telecom100k/psx_65.0_2024-01-01 00:50:00.csv')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "current_time = start_time\n",
    "\n",
    "# Читаю сохраненую закладу времени\n",
    "if Path(settings.time_pointer_file).exists():\n",
    "    with open(Path(settings.time_pointer_file), 'rt') as time_pointer_file:\n",
    "        current_time_pointer = pd.Timestamp(time_pointer_file.readline(), tz=current_timezone)\n",
    "else:\n",
    "    current_time_pointer = pd.Timestamp('1900-01-01 00:00:00', tz=current_timezone)\n",
    "\n",
    "# Перебираем циклом каждый временной период по всем наборам источников за все время, выбираем одно время за раз\n",
    "for i, (r0_file, r1_file, r2_file, r3_file, r4_file, r5_file) in enumerate(zip(r0_logs, r1_logs, r2_logs, r3_logs, r4_logs, r5_logs)):\n",
    "    logger.info(f\"Итерация: {i}, время: {current_time}\")\n",
    "    next_time = current_time + pd.Timedelta(minutes=1)\n",
    "\n",
    "    # Берем поправку на время данных\n",
    "    df_time = pd.Timestamp(r0_file.stem[9:], tz=current_timezone)\n",
    "    df_delta = datetime.now(tz=current_timezone) - df_time\n",
    "\n",
    "    # Пропускаем уже прочитанное время\n",
    "    if df_time < current_time_pointer:\n",
    "        logger.debug(f\"Пропускаю время: {df_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        continue\n",
    "    \n",
    "    # берем txt источники\n",
    "    for df_file in (r0_file, r1_file, r2_file):\n",
    "        logger.debug(f\"{df_file=}\")\n",
    "        df = pd.read_csv(df_file, sep='|', parse_dates=['StartSession','EndSession'], dayfirst=True).rename(columns={\"Duartion\": \"Duration\"})\n",
    "        df['StartSession'] = df['StartSession'].dt.tz_localize('Etc/GMT-5').dt.tz_convert(current_timezone) + df_delta\n",
    "        df['EndSession'] = df['EndSession'].dt.tz_localize('Etc/GMT-5').dt.tz_convert(current_timezone) + df_delta\n",
    "\n",
    "        send_dataframe(dataframe=df, model=Router, producer=producer, headers=[('df_file', df_file.encode('utf8')),])\n",
    "    # Берем csv источники\n",
    "    for df_file in (r3_file, r4_file, r5_file):\n",
    "        logger.debug(f\"{df_file=}\")\n",
    "        df = pd.read_csv(df_file, sep=',', parse_dates=['StartSession','EndSession'], dayfirst=True).rename(columns={\"Duartion\": \"Duration\"})\n",
    "        df['StartSession'] = df['StartSession'].dt.tz_localize('Etc/GMT-6').dt.tz_convert(current_timezone) + df_delta\n",
    "        df['EndSession'] = df['EndSession'].dt.tz_localize('Etc/GMT-6').dt.tz_convert(current_timezone) + df_delta\n",
    "\n",
    "        send_dataframe(dataframe=df, model=Router, producer=producer, headers=[('df_file', df_file.encode('utf8')),])\n",
    "    producer.flush()\n",
    "\n",
    "    # Записываем текущую временную метку\n",
    "    with open(Path(settings.time_pointer_file), 'wt') as time_pointer_file:\n",
    "        print((df_time + pd.Timedelta(minutes=10)).strftime('%Y-%m-%d %H:%M:%S'), file=time_pointer_file)\n",
    "\n",
    "    # Ожидаем следующий перод времени\n",
    "    wait_until(next_time)\n",
    "    \n",
    "    # Ограничиваем глубину вывода для отладки\n",
    "    # if i > 2:\n",
    "    #     break\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82a0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
