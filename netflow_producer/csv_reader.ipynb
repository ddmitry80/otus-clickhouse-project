{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cdc5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from kafka import KafkaProducer\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "from typing import Any, Dict\n",
    "import pytz\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "\n",
    "\n",
    "# Модель данных netflow\n",
    "class Router(BaseModel):\n",
    "    IdSession: int\n",
    "    IdPSX: int\n",
    "    IdSubscriber: int\n",
    "    StartSession: datetime\n",
    "    EndSession: datetime | None\n",
    "    Duration: int\n",
    "    UpTx: int\n",
    "    DownTx: int\n",
    "    SourceFile: str\n",
    "\n",
    "    @field_validator('EndSession', mode='before')\n",
    "    def convert_nat_to_none(cls, v: object) -> object:\n",
    "        if v is pd.NaT:\n",
    "            return None\n",
    "        return v\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    model_config = SettingsConfigDict(env_ignore_empty=True, env_file='.env', env_file_encoding='utf-8')\n",
    "\n",
    "    current_timezone: str = 'Europe/Moscow'\n",
    "    kafka_broker: str = 'localhost:9092'\n",
    "    kafka_topic: str = 'csv_data_topic'\n",
    "    csv_directory: str = '../data/TelecomX/telecom100k/'\n",
    "    log_file: str = \"logs/netflow_producer.log\"\n",
    "    time_pointer_file: str = 'logs/time_pointer.json'\n",
    "    wait_time: int = 10  # время ожидания перед отправкой следующей порции данных\n",
    "\n",
    "\n",
    "settings = Settings()\n",
    "\n",
    "logger.add(settings.log_file)\n",
    "\n",
    "KAFKA_BROKER = settings.kafka_broker\n",
    "KAFKA_TOPIC = settings.kafka_topic\n",
    "CSV_DIRECTORY = settings.csv_directory\n",
    "\n",
    "current_timezone = pytz.timezone(settings.current_timezone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3730fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kafka_producer() -> KafkaProducer:\n",
    "    \"\"\"\n",
    "    Инициализируем KafkaProducer с базовыми настройками.\n",
    "    В продакшене нужно добавить обработку ошибок подключения, настройки безопасности и т.д.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        producer = KafkaProducer(\n",
    "            bootstrap_servers=KAFKA_BROKER,\n",
    "            # value_serializer=lambda v: json.dumps(v, allow_nan=False).encode('utf-8'),\n",
    "            value_serializer=lambda v: str(v).encode('utf-8'),\n",
    "            retries=5,               # Повторная отправка при сбоях\n",
    "            linger_ms=10,            # Небольшая задержка перед отправкой\n",
    "            max_request_size=1048576 # Ограничение размера запроса (1MB)\n",
    "        )\n",
    "        logger.info(\"KafkaProducer успешно инициализирован!\")\n",
    "        return producer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка инициализации KafkaProducer: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "567ba346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_until(target_time: datetime):\n",
    "    \"\"\"Ждёт до указанного времени (формат 'HH:MM:SS').\"\"\"\n",
    "    while True:\n",
    "        now = datetime.now()\n",
    "        if now >= target_time:\n",
    "            break  # Если время уже наступило, выходим\n",
    "        time_left = (target_time - now).total_seconds()\n",
    "        time.sleep(min(time_left, 1))  # Ждём не более 1 секунды за раз\n",
    "\n",
    "\n",
    "def send_dataframe(dataframe: pd.DataFrame, model: BaseModel, producer: KafkaProducer, headers: dict = None):\n",
    "    \"\"\"Отправка датафрейма Pandas в топик Kafka\"\"\"\n",
    "    headers_list = [(key, str(headers[key]).encode('utf8')) for key in headers]\n",
    "    for rec in dataframe.to_dict(orient='records'):\n",
    "        # print(f\"{rec=}\")\n",
    "        producer.send(KAFKA_TOPIC, value=model(**rec).model_dump_json(), headers=headers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4945ebb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1564 entries, 0 to 1563\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype                        \n",
      "---  ------        --------------  -----                        \n",
      " 0   IdSession     1564 non-null   int64                        \n",
      " 1   IdPSX         1564 non-null   int64                        \n",
      " 2   IdSubscriber  1564 non-null   int64                        \n",
      " 3   StartSession  1564 non-null   datetime64[ns, Europe/Moscow]\n",
      " 4   EndSession    20 non-null     datetime64[ns, Europe/Moscow]\n",
      " 5   Duration      1564 non-null   int64                        \n",
      " 6   UpTx          1564 non-null   int64                        \n",
      " 7   DownTx        1564 non-null   int64                        \n",
      "dtypes: datetime64[ns, Europe/Moscow](2), int64(6)\n",
      "memory usage: 97.9 KB\n",
      "df_time=Timestamp('2024-01-01 00:10:00+0300', tz='Europe/Moscow')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39456/2827315887.py:11: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df0 = pd.read_csv(csv_file0, parse_dates=['StartSession','EndSession'],sep='|').rename(columns={\"Duartion\": \"Duration\"})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      2025-05-01 05:07:58.230541+03:00\n",
       "1      2025-05-01 05:26:41.230541+03:00\n",
       "2      2025-05-01 14:39:01.230541+03:00\n",
       "3      2025-05-01 16:05:22.230541+03:00\n",
       "4      2025-05-01 07:26:51.230541+03:00\n",
       "                     ...               \n",
       "1559   2025-05-01 15:59:33.230541+03:00\n",
       "1560   2025-05-01 14:18:51.230541+03:00\n",
       "1561   2025-05-01 13:21:04.230541+03:00\n",
       "1562   2025-05-01 09:24:47.230541+03:00\n",
       "1563   2025-05-01 15:24:11.230541+03:00\n",
       "Name: StartSession, Length: 1564, dtype: datetime64[ns, Europe/Moscow]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r0_logs = sorted(Path(CSV_DIRECTORY).glob('psx_66.1_*.txt'))\n",
    "r1_logs = sorted(Path(CSV_DIRECTORY).glob('psx_66.2_*.txt'))\n",
    "r2_logs = sorted(Path(CSV_DIRECTORY).glob('psx_66.3_*.txt'))\n",
    "r3_logs = sorted(Path(CSV_DIRECTORY).glob('psx_62.0_*.csv'))\n",
    "r4_logs = sorted(Path(CSV_DIRECTORY).glob('psx_69.0_*.csv'))\n",
    "r5_logs = sorted(Path(CSV_DIRECTORY).glob('psx_65.0_*.csv'))\n",
    "\n",
    "# r3_logs[0:10]\n",
    "# csv_file1 = Path(CSV_DIRECTORY+'psx_65.0_2024-01-04 08:20:00.csv');\n",
    "csv_file0 = r0_logs[0]\n",
    "df0 = pd.read_csv(csv_file0, parse_dates=['StartSession','EndSession'],sep='|').rename(columns={\"Duartion\": \"Duration\"})\n",
    "df0['StartSession'] = df0['StartSession'].dt.tz_localize('Etc/GMT-5').dt.tz_convert('Europe/Moscow')\n",
    "df0['EndSession'] = df0['EndSession'].dt.tz_localize('Etc/GMT-5').dt.tz_convert('Europe/Moscow')\n",
    "df0.info()\n",
    "\n",
    "df_time = pd.Timestamp(csv_file0.stem[9:], tz=current_timezone)\n",
    "print(f'{df_time=}')\n",
    "df_delta = datetime.now(tz=current_timezone) - df_time\n",
    "df0['StartSession'].add(df_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd9c5272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-01 19:10:17.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_kafka_producer\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mKafkaProducer успешно инициализирован!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Список фалов-источников\n",
    "r0_logs = sorted(Path(CSV_DIRECTORY).glob('psx_66.1_*.txt'))\n",
    "r1_logs = sorted(Path(CSV_DIRECTORY).glob('psx_66.2_*.txt'))\n",
    "r2_logs = sorted(Path(CSV_DIRECTORY).glob('psx_66.3_*.txt'))\n",
    "r3_logs = sorted(Path(CSV_DIRECTORY).glob('psx_62.0_*.csv'))\n",
    "r4_logs = sorted(Path(CSV_DIRECTORY).glob('psx_69.0_*.csv'))\n",
    "r5_logs = sorted(Path(CSV_DIRECTORY).glob('psx_65.0_*.csv'))\n",
    "\n",
    "producer = get_kafka_producer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24b5fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_state(path: Path | str, current_time: pd.Timestamp, time_pointer: pd.Timestamp):\n",
    "    \"\"\"Сохраняет текущее время и поток в файл\"\"\"\n",
    "    record = {\n",
    "        'current_time': current_time.isoformat() #.strftime('%Y-%m-%d %H:%M:%S'), \n",
    "        , 'time_pointer': time_pointer.isoformat() #.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "    with open(Path(path), 'wt') as state_file:\n",
    "        state_file.write(json.dumps(record))\n",
    "    logger.debug('State has saved')\n",
    "\n",
    "\n",
    "def load_state(path: Path | str, tz: pytz.BaseTzInfo) -> tuple[pd.Timestamp, pd.Timestamp]:\n",
    "    \"\"\"Возвращает текущее время и поток (current_time, time_pointer) из файла\"\"\"\n",
    "    path = Path(path)\n",
    "    with open(path, 'rt') as state_file:\n",
    "        current_state = json.loads(state_file.readline())\n",
    "    logger.debug('State has loaded')\n",
    "    return pd.Timestamp(current_state['current_time'], tz=tz), pd.Timestamp(current_state['time_pointer'], tz=tz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ca7fa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-01 19:29:07.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mИтерация: 0, время: 2025-05-01 19:29:07.926932+03:00\u001b[0m\n",
      "\u001b[32m2025-05-01 19:29:07.932\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1mdf_file=PosixPath('../data/TelecomX/telecom100k/psx_66.1_2024-01-01 00:10:00.txt')\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceFile\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_file\u001b[38;5;241m.\u001b[39mname;\n\u001b[1;32m     43\u001b[0m     send_dataframe(dataframe\u001b[38;5;241m=\u001b[39mdf, model\u001b[38;5;241m=\u001b[39mRouter, producer\u001b[38;5;241m=\u001b[39mproducer, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_file\u001b[39m\u001b[38;5;124m'\u001b[39m: df_file})\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_time\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Берем csv источники\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df_file \u001b[38;5;129;01min\u001b[39;00m (r3_file, r4_file, r5_file):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Управление задержками передачи\n",
    "# start_time = datetime.now()\n",
    "# current_time = start_time\n",
    "if Path(settings.time_pointer_file).exists():\n",
    "    current_time, current_time_pointer = load_state(settings.time_pointer_file, tz=current_timezone)\n",
    "else:\n",
    "    current_time = pd.Timestamp(datetime.now(), tz=current_timezone)\n",
    "    current_time_pointer = pd.Timestamp('1900-01-01 00:00:00', tz=current_timezone)\n",
    "\n",
    "# Читаю сохраненую закладу времени\n",
    "# save_state(settings.time_pointer_file)\n",
    "# if Path(settings.time_pointer_file).exists():\n",
    "#     with open(Path(settings.time_pointer_file), 'rt') as time_pointer_file:\n",
    "#         current_time_pointer = pd.Timestamp(time_pointer_file.readline(), tz=current_timezone)\n",
    "# else:\n",
    "#     current_time_pointer = pd.Timestamp('1900-01-01 00:00:00', tz=current_timezone)\n",
    "\n",
    "\n",
    "# Перебираем циклом каждый временной период по всем наборам источников за все время, выбираем одно время за раз\n",
    "for i, (r0_file, r1_file, r2_file, r3_file, r4_file, r5_file) in enumerate(zip(r0_logs, r1_logs, r2_logs, r3_logs, r4_logs, r5_logs)):\n",
    "    logger.info(f\"Итерация: {i}, время: {current_time}\")\n",
    "    \n",
    "    next_time = current_time + pd.Timedelta(minutes=settings.wait_time)\n",
    "\n",
    "    # Берем поправку на время данных\n",
    "    df_time = pd.Timestamp(r0_file.stem[9:], tz=current_timezone)\n",
    "    df_delta = current_time - df_time\n",
    "\n",
    "    # Пропускаем уже прочитанное время\n",
    "    if df_time < current_time_pointer:\n",
    "        logger.debug(f\"Пропускаю время: {df_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        # current_time = next_time\n",
    "        continue\n",
    "    \n",
    "    # берем txt источники\n",
    "    for df_file in (r0_file, r1_file, r2_file):\n",
    "        logger.debug(f\"{df_file=}\")\n",
    "        df = pd.read_csv(df_file, sep='|', parse_dates=['StartSession','EndSession'], dayfirst=True).rename(columns={\"Duartion\": \"Duration\"})\n",
    "        df['StartSession'] = df['StartSession'].dt.tz_localize('Etc/GMT-5').dt.tz_convert(current_timezone) + df_delta\n",
    "        df['EndSession'] = df['EndSession'].dt.tz_localize('Etc/GMT-5').dt.tz_convert(current_timezone) + df_delta\n",
    "        df['SourceFile'] = df_file.name;\n",
    "\n",
    "        send_dataframe(dataframe=df, model=Router, producer=producer, headers={'df_file': df_file})\n",
    "        time.sleep(settings.wait_time*60/10)\n",
    "    # Берем csv источники\n",
    "    for df_file in (r3_file, r4_file, r5_file):\n",
    "        logger.debug(f\"{df_file=}\")\n",
    "        df = pd.read_csv(df_file, sep=',', parse_dates=['StartSession','EndSession'], dayfirst=True).rename(columns={\"Duartion\": \"Duration\"})\n",
    "        df['StartSession'] = df['StartSession'].dt.tz_localize('Etc/GMT-6').dt.tz_convert(current_timezone) + df_delta\n",
    "        df['EndSession'] = df['EndSession'].dt.tz_localize('Etc/GMT-6').dt.tz_convert(current_timezone) + df_delta\n",
    "        df['SourceFile'] = df_file.name;\n",
    "\n",
    "        send_dataframe(dataframe=df, model=Router, producer=producer, headers={'df_file': df_file})\n",
    "        time.sleep(settings.wait_time*60/10)\n",
    "    producer.flush()\n",
    "\n",
    "    # Сохраняю текущее состояние\n",
    "    save_state(\n",
    "        path = settings.time_pointer_file \n",
    "        ,current_time = next_time\n",
    "        ,time_pointer = df_time +  pd.Timedelta(minutes=10)\n",
    "        ,tz = settings.current_timezone\n",
    "        )\n",
    "    \n",
    "    # # Записываем текущую временную метку\n",
    "    # with open(Path(settings.time_pointer_file), 'wt') as time_pointer_file:\n",
    "    #     print((df_time + pd.Timedelta(minutes=10)).strftime('%Y-%m-%d %H:%M:%S'), file=time_pointer_file)\n",
    "\n",
    "    # Ожидаем следующий перод времени\n",
    "    wait_until(next_time)\n",
    "    current_time = next_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c82a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = Router(IdSession=1, IdPSX=2, IdSubscriber=3, StartSession=pd.Timestamp('2024-01-01'), EndSession=None, Duration=10, UpTx=1, DownTx=1, SourceFile='abc_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fad3c172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"IdSession\":1,\"IdPSX\":2,\"IdSubscriber\":3,\"StartSession\":\"2024-01-01T00:00:00\",\"EndSession\":null,\"Duration\":10,\"UpTx\":1,\"DownTx\":1,\"SourceFile\":\"abc_file\"}'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.model_dump_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c84940ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IdSession': 1,\n",
       " 'IdPSX': 2,\n",
       " 'IdSubscriber': 3,\n",
       " 'StartSession': Timestamp('2024-01-01 00:00:00'),\n",
       " 'EndSession': None,\n",
       " 'Duration': 10,\n",
       " 'UpTx': 1,\n",
       " 'DownTx': 1,\n",
       " 'SourceFile': 'abc_file'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a89a2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16     2025-05-01 17:06:28.187621+03:00\n",
       "82     2025-05-01 17:01:36.187621+03:00\n",
       "113    2025-05-01 17:01:41.187621+03:00\n",
       "214    2025-05-01 17:01:57.187621+03:00\n",
       "231    2025-05-01 17:07:41.187621+03:00\n",
       "273    2025-05-01 17:03:52.187621+03:00\n",
       "360    2025-05-01 17:02:00.187621+03:00\n",
       "488    2025-05-01 17:08:23.187621+03:00\n",
       "538    2025-05-01 17:08:58.187621+03:00\n",
       "742    2025-05-01 17:07:23.187621+03:00\n",
       "743    2025-05-01 17:02:04.187621+03:00\n",
       "800    2025-05-01 17:05:17.187621+03:00\n",
       "1055   2025-05-01 17:08:33.187621+03:00\n",
       "1180   2025-05-01 17:02:31.187621+03:00\n",
       "1327   2025-05-01 17:00:31.187621+03:00\n",
       "1335   2025-05-01 17:04:39.187621+03:00\n",
       "1371   2025-05-01 17:05:28.187621+03:00\n",
       "1398   2025-05-01 17:04:55.187621+03:00\n",
       "1470   2025-05-01 17:04:07.187621+03:00\n",
       "1546   2025-05-01 17:06:13.187621+03:00\n",
       "Name: EndSession, dtype: datetime64[ns, Europe/Moscow]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['EndSession'][df['EndSession'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d043608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
